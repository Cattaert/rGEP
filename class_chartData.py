"""
Created by:      Bryce Chung
Last modified:   January 4, 2016

Description:     This class can upload and save data from text files generated by AnimatLab
                 simulations. The intention is to provide a standardized object/interface
                 through which customized data analysis methods can be written. This way,
                 the user can focus on writing analysis scripts rather than data manage-
                 ment code.
                 
                 The class can also "compress" data that is saved as "spikes".
                 Currently, the object can handle two data types from AnimatLab: "analog"
                 and "spikes." Analog corresponds to position or velocity data where a signal
                 is considered "continuous." "Spikes" is a type of data channel that records
                 binary events as 0's or 1's.
Translated in Python 3.8 Jan 2023 (D. Cattaert)
"""

import os, glob
import csv

import traceback

import numpy as np

import multiprocessing
import pickle

global verbose
verbose = 3

## ===== ===== ===== ===== =====
## ===== ===== ===== ===== =====

class chartData(object):
    """
    chartData(chartName)
    
    chartName     Unique name for the chart
    
    Manages analysis of data for visualization and analysis. This class can also
    manage several data files at once.
    """
    
    def __init__(self, chartName):
        """
        __init__(chartName)
        
        chartName     Unique name for the chart
        
        Initialize a chartData object with a file name string or a list of file
        name strings.
        
        """
        
        self.name = chartName
    
    def _readCSV(self, queue, filename, analogChans, verb=1):
        """
        _readCSV(queue, filename, analogChans, verb=1
        
        queue          Pipe-type object used by multiprocessing for parallelizing program
        filename       Name of CSV data file
        analogChans    List of channels that are analog data
        verb           Verbose setting for debugging and output
        
        This member function is spawned as a child process to allocate
        additional memory to read CSV files and to release that memory once the
        file has been read.
        """
        # Load CSV file
        if verb > 0:
            print("\nLoading CSV data file: %s" % filename)
            
        csvData = []
        with open(filename, 'r') as csvFile:
            csv.reader(csvFile)
            for row in csvFile:
                csvData.append(row.replace('\n','').split('\t'))
                
        csvFile.close()

        # Filter and format CSV data
        spikeColNames = csvData[0]
        if verb > 1:
            print("\nScrubbing data for null values...")
            
        delCols = []
        for ix, col in enumerate(spikeColNames):
            if verb > 2:
                print("Col: %s" % col)
            
            if col == '':
                delCols.append(ix)
                
        spikeData = np.delete(csvData, delCols, 1)[1:]
        spikeColNames = np.delete(spikeColNames, delCols)
        
        data = {}
        data['data'] = np.array(spikeData).astype(float).T
        data['spikeColNames'] = spikeColNames
        data['analogChans'] = analogChans        
        
        queue.put((os.path.split(filename)[-1].split('.')[0], data))

        
    def get_source(self, dataSource, analogChans=[], saveCSV=True, asDaemon=True):
        """
        Set data source.
        
        Supply a list of channels to save as "analog data" without filtering to
        spike times.
        """
        if type(dataSource) is str:
            dataSource = [dataSource]

        if verbose > 1:
            print("\n\nGenerating chartData object!")
                    
        self.rawData = {}
        
        for d in dataSource:
            if asDaemon:
                try:
                    q = multiprocessing.Queue()
                    print("Opened multiprocessing.Queue")
                    p = multiprocessing.Process(target=self._readCSV, args=((q,d,analogChans,verbose)))
                    print("Set up multiprocessing.Process")
                    p.start()
                    
                    print("Getting q data")
                    data = q.get()
                    
                    print("Closing queue")
                    q.close()
                    q.join_thread()
                    p.join()
                    
                except:
                    print(traceback.format_exc())
                    raise            
            else:
                try:
                    # Load CSV file
                    if verbose > 0:
                        print("\nLoading CSV data file: %s" % d)
                        
                    csvData = []
                    with open(d, 'r') as csvFile:
                        csv.reader(csvFile)
                        for row in csvFile:
                            csvData.append(row.replace('\n','').split('\t'))
                            
                    csvFile.close()
            
                    # Filter and format CSV data
                    spikeColNames = csvData[0]
                    if verbose > 1:
                        print("\nScrubbing data for null values...")
                        
                    delCols = []
                    for ix, col in enumerate(spikeColNames):
                        if verbose > 2:
                            print("Col: %s" % col)
                        
                        if col == '':
                            delCols.append(ix)
                            
                    spikeData = np.delete(csvData, delCols, 1)[1:]
                    spikeColNames = np.delete(spikeColNames, delCols)
                    
                    tempData = {}
                    tempData['data'] = np.array(spikeData).astype(float).T
                    tempData['spikeColNames'] = spikeColNames
                    tempData['analogChans'] = analogChans        
                    
                    data = (os.path.split(d)[-1].split('.')[0], tempData)
                    
                except:
                    if verbose > 2:
                        print(traceback.format_exc())
                        raise
                    
        
            self.rawData[data[0]] = data[1]
                
            if not saveCSV:
                os.remove(d)
            
            
    def compress(self, saveRawData=False):
        """
        Filter data to save only spike times and analog data channels.
        """
        if self.rawData is None:
            print("WARNING: No raw data to compress.")
            return
        
        self.data = {}
        for d in self.rawData:
            spikeData = self.rawData[d]['data']
            spikeColNames = self.rawData[d]['spikeColNames']
            analogChans = self.rawData[d]['analogChans']
            
            if verbose > 1:
                print("\nFormatting data to save memory!")
            
            for ix, col in enumerate(spikeColNames):
                if verbose > 1:
                    print("\n\nProcessing column: %s" % spikeColNames[ix])
                    
                tempData = {}
                if (col in analogChans) or (col in ['Time', 'TimeSlice']):
                    if verbose > 1:
                        print("Channel type: Analog")
                    tempData['data'] = spikeData[ix]
                    tempData['datatype'] = 'analog'
                else:
                    if verbose > 1:
                        print("Channel type: Spike")
                    tempData['data'] = spikeData[1][np.where(spikeData[ix] == 1)[0]]
                    tempData['datatype'] = 'spike'
                    
                if verbose > 2:
                    print("Data channel size: %i" % len(tempData['data']))
                    
                if col in self.data.keys():
                    if verbose > 2:
                        print("Repeated column heading: %s" % col)
                        
                    flag = False
                    try:
                        if not (self.data[col]['data'] == tempData['data']).all():
                            flag = True
                    except:
                        try:
                            if not self.data[col]['data'] == tempData['data']:
                                flag = True
                        except:
                            raise
                    if flag:
                        count = len(np.where(np.array(list(self.data.keys())) == col))
                        self.data[col+'-%i' % count] = tempData
                    else:
                        self.data[col] = tempData
                else:
                    self.data[col] = tempData
                    
        if not saveRawData:
            self.rawData = None
        
    
    def saveData(self, filename='', overwrite=False):
        if filename == '':
            filename = 'chartData_%s.dat' % self.name
        if overwrite and os.path.isfile(filename):
            os.remove(filename)
            raise ValueError("File already exists!")
        
        pickle.dump(self.data, open(filename, 'wb'))
        
    
    def loadData(self, filename=''):
        if filename == '':
            filename = 'chartData_%s.dat' % self.name
        self.data = pickle.load(open(filename, 'rb'))
